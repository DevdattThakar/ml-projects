{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMNeUGh8oGLStHfTg/DwhHj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"TFKdgGmgppF_","executionInfo":{"status":"ok","timestamp":1763397939487,"user_tz":-330,"elapsed":32,"user":{"displayName":"DEVDATT THAKAR","userId":"07526693799587793914"}}},"outputs":[],"source":["# Creating two portfolio-ready Jupyter notebooks (IPYNB) for Colab:\n","# 1) MNIST Handwritten Digit Recognizer (portfolio-ready)\n","# 2) MovieLens 100k Movie Recommender (portfolio-ready)\n","# Files will be saved to /mnt/data and displayed for download.\n","import nbformat as nbf, os, textwrap, json\n","\n","OUT_DIR = \"/mnt/data/colab_notebooks_for_vaishali\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","def make_notebook(cells, path):\n","    nb = nbf.v4.new_notebook()\n","    nb['cells'] = cells\n","    with open(path, 'w', encoding='utf-8') as f:\n","        nbf.write(nb, f)\n","\n","# Notebook 1: MNIST (portfolio-ready)\n","cells_mnist = []\n","\n","cells_mnist.append(nbf.v4.new_markdown_cell(\"# Handwritten Digit Recognizer (MNIST)\\n\\n**Portfolio-ready Colab notebook**\\n\\n**What you'll find:** problem statement, approach, full code (TensorFlow/Keras), training, evaluation, sample predictions, and suggestions for improvement.\\n\\n---\"))\n","\n","cells_mnist.append(nbf.v4.new_markdown_cell(\"## 1) Setup & Dependencies\\nRun the cell below to install required packages (on Colab `tensorflow` is typically preinstalled, but we include the command for reproducibility).\"))\n","\n","cells_mnist.append(nbf.v4.new_code_cell(\"!pip install -q tensorflow numpy matplotlib\\nimport tensorflow as tf\\nprint('TensorFlow version:', tf.__version__)\"))\n","\n","cells_mnist.append(nbf.v4.new_markdown_cell(\"## 2) Problem Statement\\nPredict the handwritten digit (0–9) from 28x28 grayscale images using a Convolutional Neural Network (CNN). We'll use the built-in MNIST dataset provided by TensorFlow.\"))\n","\n","cells_mnist.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","# 3) Load data and visualize samples\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","\n","(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","x_train = x_train.astype('float32') / 255.0\n","x_test = x_test.astype('float32') / 255.0\n","\n","# show a grid of sample images with labels\n","fig, axes = plt.subplots(2,5, figsize=(10,4))\n","for i, ax in enumerate(axes.flatten()):\n","    ax.imshow(x_train[i], cmap='gray')\n","    ax.set_title(f'label: {y_train[i]}')\n","    ax.axis('off')\n","plt.show()\"\"\")))\n","\n","cells_mnist.append(nbf.v4.new_markdown_cell(\"## 4) Build the CNN model\\nA compact CNN suitable for MNIST. You can increase depth or add augmentation for better accuracy.\"))\n","\n","cells_mnist.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","from tensorflow.keras import layers, models\n","\n","def build_model():\n","    model = models.Sequential([\n","        layers.Input(shape=(28,28,1)),\n","        layers.Reshape((28,28,1)),\n","        layers.Conv2D(32, (3,3), activation='relu'),\n","        layers.MaxPooling2D((2,2)),\n","        layers.Conv2D(64, (3,3), activation='relu'),\n","        layers.MaxPooling2D((2,2)),\n","        layers.Flatten(),\n","        layers.Dense(128, activation='relu'),\n","        layers.Dropout(0.4),\n","        layers.Dense(10, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n","\n","model = build_model()\n","model.summary()\"\"\")))\n","\n","cells_mnist.append(nbf.v4.new_markdown_cell(\"## 5) Training\\nUse a small number of epochs for quick runs; increase for better accuracy. The notebook will save the model to `model_mnist.h5`. You can also use Colab GPU runtime for faster training (Runtime → Change runtime type → GPU).\"))\n","\n","cells_mnist.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","# Prepare data shapes\n","x_train_in = np.expand_dims(x_train, -1)\n","x_test_in = np.expand_dims(x_test, -1)\n","\n","# Train\n","history = model.fit(x_train_in, y_train, validation_split=0.1, epochs=5, batch_size=128)\n","\n","# Evaluate\n","loss, acc = model.evaluate(x_test_in, y_test, verbose=2)\n","print(f'Test accuracy: {acc:.4f}')\n","\n","# Save model\n","model.save('model_mnist.h5')\n","print('Saved model to model_mnist.h5')\"\"\")))\n","\n","cells_mnist.append(nbf.v4.new_markdown_cell(\"## 6) Training curves\\nPlot accuracy and loss during training.\"))\n","\n","cells_mnist.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12,4))\n","plt.subplot(1,2,1)\n","plt.plot(history.history['loss'], label='train_loss')\n","plt.plot(history.history['val_loss'], label='val_loss')\n","plt.legend()\n","plt.title('Loss')\n","\n","plt.subplot(1,2,2)\n","plt.plot(history.history['accuracy'], label='train_acc')\n","plt.plot(history.history['val_accuracy'], label='val_acc')\n","plt.legend()\n","plt.title('Accuracy')\n","plt.show()\"\"\")))\n","\n","cells_mnist.append(nbf.v4.new_markdown_cell(\"## 7) Sample Predictions\\nLoad the saved model and predict on a few test images.\"))\n","\n","cells_mnist.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","model2 = load_model('model_mnist.h5')\n","\n","# Predict first 10 test images\n","preds = model2.predict(x_test_in[:10])\n","pred_labels = preds.argmax(axis=1)\n","for i in range(10):\n","    print(f'Index {i}: predicted={pred_labels[i]}, true={y_test[i]}')\"\"\")))\n","\n","cells_mnist.append(nbf.v4.new_markdown_cell(\"## 8) Results & Conclusions\\n- This simple CNN should reach ~98% accuracy with 5–10 epochs. Results may vary.\\n- **Future improvements:** data augmentation, batch normalization, deeper network, hyperparameter tuning, or transfer learning.\\n\\n---\\n\\n*Download this notebook and open it in Colab (File → Upload notebook) or run it directly by copying the cells into a Colab notebook.*\"))\n","\n","mnist_path = os.path.join(OUT_DIR, \"MNIST_Handwritten_Digit_Recognizer_Portfolio.ipynb\")\n","make_notebook(cells_mnist, mnist_path)"]},{"cell_type":"code","source":["cells_mov = []\n","cells_mov.append(nbf.v4.new_markdown_cell(\"# Movie Recommendation System (MovieLens 100K)\\n\\n**Portfolio-ready Colab notebook**\\n\\nA simple item-based collaborative filtering recommender using the MovieLens 100k dataset. Includes data loading, exploratory analysis, a baseline recommender, evaluation (RMSE), and top-N recommendations.\\n\\n---\"))\n","\n","cells_mov.append(nbf.v4.new_markdown_cell(\"## 1) Setup & Dependencies\\nInstall required packages. The notebook will download the MovieLens 100k dataset at runtime.\"))\n","\n","cells_mov.append(nbf.v4.new_code_cell(\"!pip install -q pandas numpy scikit-learn\\nimport pandas as pd, numpy as np\\nprint('Ready')\"))\n","\n","cells_mov.append(nbf.v4.new_markdown_cell(\"## 2) Download and Load Data\\nWe will download the MovieLens 100k zip from GroupLens and load `u.data`. The dataset contains user-item ratings (1–5).\"))\n","\n","cells_mov.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","import os, urllib.request, zipfile, io\n","DATA_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n","fn = 'ml-100k.zip'\n","if not os.path.exists('ml-100k'):\n","    print('Downloading MovieLens 100k...')\n","    r = urllib.request.urlopen(DATA_URL)\n","    z = zipfile.ZipFile(io.BytesIO(r.read()))\n","    z.extractall()\n","    print('Extracted to ml-100k/')\n","else:\n","    print('MovieLens already present.')\n","\n","# Load u.data\n","cols = ['user_id', 'item_id', 'rating', 'timestamp']\n","df = pd.read_csv('ml-100k/u.data', sep='\\\\t', names=cols)\n","df.head()\"\"\")))\n","\n","cells_mov.append(nbf.v4.new_markdown_cell(\"## 3) Exploratory Analysis\\nQuick look at dataset size and rating distribution.\"))\n","\n","cells_mov.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","print('Total ratings:', len(df))\n","print('Unique users:', df['user_id'].nunique())\n","print('Unique items:', df['item_id'].nunique())\n","\n","# rating counts\n","df['rating'].value_counts().sort_index().plot(kind='bar', title='Rating distribution')\"\"\")))\n","\n","cells_mov.append(nbf.v4.new_markdown_cell(\"## 4) Build user-item matrix and item-based recommender\\nWe compute item-item cosine similarity and score items for a user based on their existing ratings.\"))\n","\n","cells_mov.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def build_user_item_matrix(df):\n","    return df.pivot(index='user_id', columns='item_id', values='rating').fillna(0)\n","\n","ratings_matrix = build_user_item_matrix(df)\n","ratings_matrix.shape\"\"\")))\n","\n","cells_mov.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","# compute item-item cosine similarity (may take a few seconds)\n","item_sim = cosine_similarity(ratings_matrix.T)\n","item_sim = pd.DataFrame(item_sim, index=ratings_matrix.columns, columns=ratings_matrix.columns)\n","item_sim.iloc[:4,:4]\"\"\")))\n","\n","cells_mov.append(nbf.v4.new_markdown_cell(\"## 5) Recommendation function\\nReturn top-N recommendations for a given user (filtering already rated items).\"))\n","\n","cells_mov.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","def item_based_recommend(user_id, ratings_matrix, item_sim, top_n=10):\n","    user_ratings = ratings_matrix.loc[user_id]\n","    scores = item_sim.dot(user_ratings) / (item_sim.sum(axis=1) + 1e-9)\n","    unrated = user_ratings[user_ratings==0].index\n","    scores = scores.loc[unrated].sort_values(ascending=False)\n","    return scores.head(top_n)\n","\n","# Example: recommend for user 1\n","top = item_based_recommend(1, ratings_matrix, item_sim, top_n=10)\n","top.head(10)\"\"\")))\n","\n","cells_mov.append(nbf.v4.new_markdown_cell(\"## 6) Evaluate (simple RMSE baseline)\\nA simple train/test split evaluation to compute RMSE for predicted ratings (baseline method). This is not a production evaluation but an illustrative baseline.\"))\n","\n","cells_mov.append(nbf.v4.new_code_cell(textwrap.dedent(\"\"\"\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","def evaluate(df):\n","    train, test = train_test_split(df, test_size=0.2, random_state=42)\n","    train_mat = train.pivot(index='user_id', columns='item_id', values='rating').fillna(0)\n","    test_mat = test.pivot(index='user_id', columns='item_id', values='rating').fillna(0)\n","    # align columns\n","    test_mat = test_mat.reindex(columns=train_mat.columns, fill_value=0)\n","    # compute item similarity on train matrix\n","    sim = cosine_similarity(train_mat.T)\n","    preds = train_mat.dot(sim) / (np.abs(sim).sum(axis=1)+1e-9)\n","    preds = pd.DataFrame(preds, index=train_mat.index, columns=train_mat.columns)\n","    trues, pred_vals = [], []\n","    for u in test_mat.index:\n","        for i in test_mat.columns:\n","            true = test_mat.at[u,i]\n","            if true>0:\n","                trues.append(true)\n","                pred_vals.append(preds.at[u,i])\n","    rmse = mean_squared_error(trues, pred_vals, squared=False)\n","    return rmse\n","\n","rmse = evaluate(df)\n","print('Baseline RMSE:', rmse)\"\"\")))\n","\n","cells_mov.append(nbf.v4.new_markdown_cell(\"## 7) Interpretability & Next Steps\\n- The simple item-based approach is easy to explain and implement.\\n- **Improvements:** matrix factorization (SVD), model-based collaborative filtering, hybrid filtering with item metadata, implicit-feedback methods, scalability optimizations, and hyperparameter tuning.\\n\\n---\\n\\n*Download this notebook and open it in Colab (File → Upload notebook) or run it directly by copying the cells into a Colab notebook.*\"))\n","\n","mov_path = os.path.join(OUT_DIR, \"MovieLens_Recommender_Portfolio.ipynb\")\n","make_notebook(cells_mov, mov_path)\n","\n","# Create a README and zip both notebooks for download\n","readme = textwrap.dedent(f\"\"\"Two portfolio-ready Colab notebooks were created and saved to:\\n\\n{OUT_DIR}\\n\\nFiles:\\n- MNIST_Handwritten_Digit_Recognizer_Portfolio.ipynb\\n- MovieLens_Recommender_Portfolio.ipynb\\n\\nDownload them and upload to Google Colab (File → Upload notebook) or add them to a GitHub repo and use 'Open in Colab' there.\\n\"\"\")\n","\n","with open(os.path.join(OUT_DIR, \"README.txt\"), \"w\") as f:\n","    f.write(readme)\n","\n","# Create zip\n","import zipfile\n","zip_path = \"/mnt/data/colab_notebooks_for_vaishali.zip\"\n","with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n","    for fn in os.listdir(OUT_DIR):\n","        zf.write(os.path.join(OUT_DIR, fn), arcname=fn)\n","\n","zip_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Jmhf2krtqAzh","executionInfo":{"status":"ok","timestamp":1763397933430,"user_tz":-330,"elapsed":27,"user":{"displayName":"DEVDATT THAKAR","userId":"07526693799587793914"}},"outputId":"ede6529c-7a6b-42c2-82b4-11ef64a6d10f"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/mnt/data/colab_notebooks_for_vaishali.zip'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":[],"metadata":{"id":"W6h1B1vUtM3z"}},{"cell_type":"markdown","source":[],"metadata":{"id":"BAD7O9dftNJm"}}]}